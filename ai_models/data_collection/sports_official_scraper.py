"""
ESPN, Olympic.org ë“± ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘
Official Sports Websites Data Scraper
"""

import os
import json
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import requests
from typing import List, Dict, Optional
import logging
from datetime import datetime
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import pandas as pd
from tqdm import tqdm
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OfficialSportsScraper:
    """ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    # ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ëª©ë¡
    OFFICIAL_SITES = {
        'olympic': {
            'base_url': 'https://olympics.com',
            'endpoints': [
                '/en/athletes',
                '/en/sports/weightlifting',
                '/en/sports/gymnastics',
                '/en/sports/athletics',
                '/en/video/sports',
                '/en/news/olympic-games'
            ]
        },
        'iwf': {  # International Weightlifting Federation
            'base_url': 'https://iwf.sport',
            'endpoints': [
                '/results',
                '/athletes',
                '/education/technique',
                '/media-centre/videos'
            ]
        },
        'ipf': {  # International Powerlifting Federation
            'base_url': 'https://www.powerlifting.sport',
            'endpoints': [
                '/championships/results',
                '/athletes',
                '/technical-corner'
            ]
        },
        'fig': {  # International Gymnastics Federation
            'base_url': 'https://www.gymnastics.sport',
            'endpoints': [
                '/athletes',
                '/technical',
                '/education'
            ]
        },
        'espn': {
            'base_url': 'https://www.espn.com',
            'endpoints': [
                '/olympics',
                '/sports/weightlifting',
                '/sports/gymnastics'
            ]
        },
        'crossfit': {
            'base_url': 'https://games.crossfit.com',
            'endpoints': [
                '/leaderboard',
                '/athletes',
                '/workouts',
                '/video'
            ]
        }
    }
    
    # ì„¸ê³„ ê¸°ë¡ ë³´ìœ ì ë° ì±”í”¼ì–¸
    WORLD_CHAMPIONS = {
        'weightlifting': {
            'men': {
                '61kg': {'name': 'Li Fabin', 'country': 'CHN', 'records': {'snatch': 145, 'clean_jerk': 175}},
                '73kg': {'name': 'Shi Zhiyong', 'country': 'CHN', 'records': {'snatch': 169, 'clean_jerk': 198}},
                '81kg': {'name': 'Lu Xiaojun', 'country': 'CHN', 'records': {'snatch': 177, 'clean_jerk': 207}},
                '96kg': {'name': 'Tian Tao', 'country': 'CHN', 'records': {'snatch': 183, 'clean_jerk': 230}},
                '109kg': {'name': 'Akbar Djuraev', 'country': 'UZB', 'records': {'snatch': 193, 'clean_jerk': 237}},
                '+109kg': {'name': 'Lasha Talakhadze', 'country': 'GEO', 'records': {'snatch': 225, 'clean_jerk': 267}}
            },
            'women': {
                '49kg': {'name': 'Hou Zhihui', 'country': 'CHN', 'records': {'snatch': 96, 'clean_jerk': 118}},
                '55kg': {'name': 'Liao Qiuyun', 'country': 'CHN', 'records': {'snatch': 103, 'clean_jerk': 129}},
                '59kg': {'name': 'Kuo Hsing-chun', 'country': 'TPE', 'records': {'snatch': 110, 'clean_jerk': 140}},
                '64kg': {'name': 'Deng Wei', 'country': 'CHN', 'records': {'snatch': 117, 'clean_jerk': 145}},
                '71kg': {'name': 'Zhang Wangli', 'country': 'CHN', 'records': {'snatch': 117, 'clean_jerk': 152}},
                '76kg': {'name': 'Neisi Dajomes', 'country': 'ECU', 'records': {'snatch': 118, 'clean_jerk': 145}}
            }
        },
        'powerlifting': {
            'men': {
                'squat': {'name': 'Ray Williams', 'record': 490, 'unit': 'kg'},
                'bench': {'name': 'Julius Maddox', 'record': 355, 'unit': 'kg'},
                'deadlift': {'name': 'Hafthor Bjornsson', 'record': 501, 'unit': 'kg'}
            },
            'women': {
                'squat': {'name': 'April Mathis', 'record': 321, 'unit': 'kg'},
                'bench': {'name': 'April Mathis', 'record': 207, 'unit': 'kg'},
                'deadlift': {'name': 'Tamara Walcott', 'record': 288, 'unit': 'kg'}
            }
        }
    }
    
    def __init__(self, output_dir: str = 'official_sports_dataset'):
        self.output_dir = output_dir
        self.session = None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(f'{output_dir}/athletes', exist_ok=True)
        os.makedirs(f'{output_dir}/competitions', exist_ok=True)
        os.makedirs(f'{output_dir}/techniques', exist_ok=True)
        os.makedirs(f'{output_dir}/records', exist_ok=True)
        
        # Chrome ë“œë¼ì´ë²„ ì„¤ì •
        self.chrome_options = Options()
        self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        
        self.collected_data = {
            'athletes': [],
            'competitions': [],
            'techniques': [],
            'videos': [],
            'records': []
        }
    
    async def scrape_olympic_website(self) -> Dict:
        """Olympic.com ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("Scraping Olympic.com...")
        
        olympic_data = {
            'athletes': [],
            'sports': [],
            'videos': [],
            'records': []
        }
        
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            
            # ì—­ë„ ì„ ìˆ˜ í˜ì´ì§€
            driver.get('https://olympics.com/en/sports/weightlifting')
            time.sleep(3)
            
            # ì„ ìˆ˜ ì •ë³´ ìˆ˜ì§‘
            athlete_elements = driver.find_elements(By.CSS_SELECTOR, '.athlete-card')
            for element in athlete_elements[:20]:  # ìƒìœ„ 20ëª…
                try:
                    name = element.find_element(By.CSS_SELECTOR, '.athlete-name').text
                    country = element.find_element(By.CSS_SELECTOR, '.country-code').text
                    
                    athlete_data = {
                        'name': name,
                        'country': country,
                        'sport': 'weightlifting',
                        'source': 'olympics.com',
                        'profile_url': element.get_attribute('href')
                    }
                    
                    # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                    athlete_details = await self.get_athlete_details(driver, athlete_data['profile_url'])
                    athlete_data.update(athlete_details)
                    
                    olympic_data['athletes'].append(athlete_data)
                    
                except Exception as e:
                    logger.error(f"Error extracting athlete data: {e}")
                    continue
            
            # ê¸°ìˆ  ë¹„ë””ì˜¤ ìˆ˜ì§‘
            driver.get('https://olympics.com/en/video/sports/weightlifting')
            time.sleep(3)
            
            video_elements = driver.find_elements(By.CSS_SELECTOR, '.video-card')
            for element in video_elements[:30]:
                try:
                    title = element.find_element(By.CSS_SELECTOR, '.video-title').text
                    url = element.get_attribute('href')
                    thumbnail = element.find_element(By.CSS_SELECTOR, 'img').get_attribute('src')
                    
                    video_data = {
                        'title': title,
                        'url': url,
                        'thumbnail': thumbnail,
                        'source': 'olympics.com',
                        'category': self.categorize_video(title)
                    }
                    
                    olympic_data['videos'].append(video_data)
                    
                except Exception as e:
                    logger.error(f"Error extracting video data: {e}")
                    continue
            
            # ì„¸ê³„ ê¸°ë¡ ìˆ˜ì§‘
            olympic_data['records'] = await self.scrape_world_records(driver)
            
            driver.quit()
            
        except Exception as e:
            logger.error(f"Olympic.com scraping failed: {e}")
        
        self.collected_data['athletes'].extend(olympic_data['athletes'])
        self.collected_data['videos'].extend(olympic_data['videos'])
        self.collected_data['records'].extend(olympic_data['records'])
        
        logger.info(f"Collected {len(olympic_data['athletes'])} athletes from Olympics.com")
        return olympic_data
    
    async def get_athlete_details(self, driver, profile_url: str) -> Dict:
        """ì„ ìˆ˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        details = {}
        
        try:
            driver.get(profile_url)
            time.sleep(2)
            
            # ë©”ë‹¬ ì •ë³´
            medals = driver.find_elements(By.CSS_SELECTOR, '.medal-count')
            if medals:
                details['medals'] = {
                    'gold': medals[0].text if len(medals) > 0 else '0',
                    'silver': medals[1].text if len(medals) > 1 else '0',
                    'bronze': medals[2].text if len(medals) > 2 else '0'
                }
            
            # ê°œì¸ ê¸°ë¡
            records = driver.find_elements(By.CSS_SELECTOR, '.personal-best')
            if records:
                details['personal_records'] = []
                for record in records:
                    details['personal_records'].append(record.text)
            
            # ê²½ë ¥ í•˜ì´ë¼ì´íŠ¸
            highlights = driver.find_elements(By.CSS_SELECTOR, '.career-highlight')
            if highlights:
                details['career_highlights'] = [h.text for h in highlights[:5]]
            
        except Exception as e:
            logger.error(f"Error getting athlete details: {e}")
        
        return details
    
    async def scrape_iwf_website(self) -> Dict:
        """International Weightlifting Federation ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("Scraping IWF website...")
        
        iwf_data = {
            'competitions': [],
            'results': [],
            'techniques': []
        }
        
        async with aiohttp.ClientSession() as session:
            # ëŒ€íšŒ ê²°ê³¼ ìˆ˜ì§‘
            try:
                async with session.get('https://iwf.sport/results') as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ìµœê·¼ ëŒ€íšŒ ê²°ê³¼
                        competitions = soup.find_all('div', class_='competition-card')
                        for comp in competitions[:10]:
                            comp_data = {
                                'name': comp.find('h3').text if comp.find('h3') else '',
                                'date': comp.find('span', class_='date').text if comp.find('span', class_='date') else '',
                                'location': comp.find('span', class_='location').text if comp.find('span', class_='location') else '',
                                'level': 'world_championship',
                                'source': 'iwf.sport'
                            }
                            
                            # ê²°ê³¼ ë§í¬
                            results_link = comp.find('a', class_='results-link')
                            if results_link:
                                comp_data['results_url'] = results_link.get('href')
                                # ìƒì„¸ ê²°ê³¼ ìˆ˜ì§‘
                                comp_results = await self.get_competition_results(session, comp_data['results_url'])
                                comp_data['results'] = comp_results
                            
                            iwf_data['competitions'].append(comp_data)
            
            except Exception as e:
                logger.error(f"IWF scraping failed: {e}")
            
            # ê¸°ìˆ  êµìœ¡ ìë£Œ ìˆ˜ì§‘
            try:
                async with session.get('https://iwf.sport/education/technique') as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        techniques = soup.find_all('article', class_='technique-article')
                        for tech in techniques:
                            tech_data = {
                                'title': tech.find('h2').text if tech.find('h2') else '',
                                'description': tech.find('p').text if tech.find('p') else '',
                                'category': self.categorize_technique(tech.find('h2').text if tech.find('h2') else ''),
                                'source': 'iwf.sport'
                            }
                            
                            # ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ë§í¬
                            media = tech.find_all('img') + tech.find_all('video')
                            tech_data['media_urls'] = [m.get('src') for m in media]
                            
                            iwf_data['techniques'].append(tech_data)
            
            except Exception as e:
                logger.error(f"IWF technique scraping failed: {e}")
        
        self.collected_data['competitions'].extend(iwf_data['competitions'])
        self.collected_data['techniques'].extend(iwf_data['techniques'])
        
        logger.info(f"Collected {len(iwf_data['competitions'])} competitions from IWF")
        return iwf_data
    
    async def get_competition_results(self, session, results_url: str) -> List[Dict]:
        """ëŒ€íšŒ ê²°ê³¼ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        results = []
        
        try:
            async with session.get(results_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ê²°ê³¼ í…Œì´ë¸” íŒŒì‹±
                    result_rows = soup.find_all('tr', class_='result-row')
                    for row in result_rows[:20]:  # ìƒìœ„ 20ëª…
                        cells = row.find_all('td')
                        if len(cells) >= 6:
                            result = {
                                'rank': cells[0].text.strip(),
                                'name': cells[1].text.strip(),
                                'country': cells[2].text.strip(),
                                'snatch': cells[3].text.strip(),
                                'clean_jerk': cells[4].text.strip(),
                                'total': cells[5].text.strip()
                            }
                            results.append(result)
        
        except Exception as e:
            logger.error(f"Error getting competition results: {e}")
        
        return results
    
    async def scrape_espn(self) -> Dict:
        """ESPN ìŠ¤í¬ì¸  ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("Scraping ESPN...")
        
        espn_data = {
            'articles': [],
            'videos': [],
            'stats': []
        }
        
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            
            # ì˜¬ë¦¼í”½ ì„¹ì…˜
            driver.get('https://www.espn.com/olympics')
            time.sleep(3)
            
            # ìµœì‹  ê¸°ì‚¬
            articles = driver.find_elements(By.CSS_SELECTOR, 'article.contentItem')
            for article in articles[:15]:
                try:
                    title = article.find_element(By.CSS_SELECTOR, 'h2').text
                    link = article.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')
                    summary = article.find_element(By.CSS_SELECTOR, '.description').text if article.find_element(By.CSS_SELECTOR, '.description') else ''
                    
                    article_data = {
                        'title': title,
                        'url': link,
                        'summary': summary,
                        'source': 'espn.com',
                        'category': self.categorize_article(title)
                    }
                    
                    # ìš´ë™ ê¸°ìˆ  ê´€ë ¨ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
                    if any(keyword in title.lower() for keyword in ['technique', 'form', 'training', 'workout', 'exercise']):
                        espn_data['articles'].append(article_data)
                
                except Exception as e:
                    continue
            
            # ë¹„ë””ì˜¤ ì½˜í…ì¸ 
            driver.get('https://www.espn.com/video/sport/olympics')
            time.sleep(3)
            
            videos = driver.find_elements(By.CSS_SELECTOR, '.video-item')
            for video in videos[:20]:
                try:
                    title = video.find_element(By.CSS_SELECTOR, '.video-title').text
                    duration = video.find_element(By.CSS_SELECTOR, '.duration').text if video.find_element(By.CSS_SELECTOR, '.duration') else ''
                    
                    video_data = {
                        'title': title,
                        'duration': duration,
                        'source': 'espn.com',
                        'category': self.categorize_video(title)
                    }
                    
                    espn_data['videos'].append(video_data)
                
                except Exception as e:
                    continue
            
            driver.quit()
            
        except Exception as e:
            logger.error(f"ESPN scraping failed: {e}")
        
        self.collected_data['videos'].extend(espn_data['videos'])
        
        logger.info(f"Collected {len(espn_data['articles'])} articles from ESPN")
        return espn_data
    
    async def scrape_crossfit_games(self) -> Dict:
        """CrossFit Games ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("Scraping CrossFit Games...")
        
        cf_data = {
            'athletes': [],
            'workouts': [],
            'leaderboard': []
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                # ì„ ìˆ˜ ë¦¬ë”ë³´ë“œ
                async with session.get('https://games.crossfit.com/leaderboard') as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ìƒìœ„ ì„ ìˆ˜ë“¤
                        athletes = soup.find_all('div', class_='athlete-row')
                        for athlete in athletes[:30]:
                            athlete_data = {
                                'rank': athlete.find('span', class_='rank').text if athlete.find('span', class_='rank') else '',
                                'name': athlete.find('span', class_='name').text if athlete.find('span', class_='name') else '',
                                'country': athlete.find('span', class_='country').text if athlete.find('span', class_='country') else '',
                                'points': athlete.find('span', class_='points').text if athlete.find('span', class_='points') else '',
                                'source': 'crossfit.com'
                            }
                            cf_data['athletes'].append(athlete_data)
                
                # WOD (Workout of the Day)
                async with session.get('https://games.crossfit.com/workouts') as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        workouts = soup.find_all('div', class_='workout-card')
                        for workout in workouts[:20]:
                            wod_data = {
                                'name': workout.find('h3').text if workout.find('h3') else '',
                                'description': workout.find('p', class_='description').text if workout.find('p', class_='description') else '',
                                'movements': [],
                                'source': 'crossfit.com'
                            }
                            
                            # ë™ì‘ ëª©ë¡
                            movements = workout.find_all('li', class_='movement')
                            wod_data['movements'] = [m.text for m in movements]
                            
                            cf_data['workouts'].append(wod_data)
            
            except Exception as e:
                logger.error(f"CrossFit Games scraping failed: {e}")
        
        self.collected_data['athletes'].extend(cf_data['athletes'])
        
        logger.info(f"Collected {len(cf_data['athletes'])} CrossFit athletes")
        return cf_data
    
    async def scrape_world_records(self, driver) -> List[Dict]:
        """ì„¸ê³„ ê¸°ë¡ ìˆ˜ì§‘"""
        records = []
        
        # ì—­ë„ ì„¸ê³„ ê¸°ë¡
        for gender, categories in self.WORLD_CHAMPIONS['weightlifting'].items():
            for weight_class, champion_data in categories.items():
                record = {
                    'sport': 'weightlifting',
                    'gender': gender,
                    'weight_class': weight_class,
                    'athlete': champion_data['name'],
                    'country': champion_data['country'],
                    'records': champion_data['records'],
                    'date_collected': datetime.now().isoformat()
                }
                records.append(record)
        
        # íŒŒì›Œë¦¬í”„íŒ… ì„¸ê³„ ê¸°ë¡
        for gender, lifts in self.WORLD_CHAMPIONS['powerlifting'].items():
            for lift_type, record_data in lifts.items():
                record = {
                    'sport': 'powerlifting',
                    'gender': gender,
                    'lift': lift_type,
                    'athlete': record_data['name'],
                    'record': f"{record_data['record']} {record_data['unit']}",
                    'date_collected': datetime.now().isoformat()
                }
                records.append(record)
        
        return records
    
    def categorize_video(self, title: str) -> str:
        """ë¹„ë””ì˜¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['snatch', 'clean', 'jerk']):
            return 'olympic_lifting'
        elif any(word in title_lower for word in ['squat', 'deadlift', 'bench']):
            return 'powerlifting'
        elif any(word in title_lower for word in ['technique', 'form', 'tutorial']):
            return 'technique'
        elif any(word in title_lower for word in ['world record', 'championship', 'olympic']):
            return 'competition'
        else:
            return 'general'
    
    def categorize_technique(self, title: str) -> str:
        """ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()
        
        if 'snatch' in title_lower:
            return 'snatch_technique'
        elif 'clean' in title_lower:
            return 'clean_technique'
        elif 'jerk' in title_lower:
            return 'jerk_technique'
        elif 'squat' in title_lower:
            return 'squat_technique'
        elif 'pull' in title_lower:
            return 'pull_technique'
        else:
            return 'general_technique'
    
    def categorize_article(self, title: str) -> str:
        """ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['technique', 'form', 'how to']):
            return 'educational'
        elif any(word in title_lower for word in ['championship', 'games', 'competition']):
            return 'competition'
        elif any(word in title_lower for word in ['training', 'workout', 'program']):
            return 'training'
        else:
            return 'news'
    
    async def download_media_content(self):
        """ìˆ˜ì§‘í•œ ë¯¸ë””ì–´ ì½˜í…ì¸  ë‹¤ìš´ë¡œë“œ"""
        logger.info("Downloading media content...")
        
        # ë¹„ë””ì˜¤ URL ë‹¤ìš´ë¡œë“œ
        for video in self.collected_data['videos']:
            if 'url' in video:
                try:
                    # yt-dlpë¥¼ ì‚¬ìš©í•œ ë‹¤ìš´ë¡œë“œ (YouTube, ê¸°íƒ€ í”Œë«í¼ ì§€ì›)
                    import yt_dlp
                    
                    ydl_opts = {
                        'outtmpl': os.path.join(self.output_dir, 'videos', '%(title)s.%(ext)s'),
                        'quiet': True,
                        'no_warnings': True,
                        'format': 'best[height<=720]'  # 720p ì´í•˜ë¡œ ì œí•œ
                    }
                    
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        ydl.download([video['url']])
                    
                    logger.info(f"Downloaded: {video.get('title', 'Unknown')}")
                
                except Exception as e:
                    logger.error(f"Failed to download video: {e}")
    
    def save_collected_data(self):
        """ìˆ˜ì§‘í•œ ë°ì´í„° ì €ì¥"""
        logger.info("Saving collected data...")
        
        # ì„ ìˆ˜ ë°ì´í„° ì €ì¥
        if self.collected_data['athletes']:
            df_athletes = pd.DataFrame(self.collected_data['athletes'])
            df_athletes.to_csv(os.path.join(self.output_dir, 'athletes', 'all_athletes.csv'), index=False)
            
            with open(os.path.join(self.output_dir, 'athletes', 'all_athletes.json'), 'w') as f:
                json.dump(self.collected_data['athletes'], f, indent=2)
        
        # ëŒ€íšŒ ë°ì´í„° ì €ì¥
        if self.collected_data['competitions']:
            with open(os.path.join(self.output_dir, 'competitions', 'all_competitions.json'), 'w') as f:
                json.dump(self.collected_data['competitions'], f, indent=2)
        
        # ê¸°ìˆ  ë°ì´í„° ì €ì¥
        if self.collected_data['techniques']:
            with open(os.path.join(self.output_dir, 'techniques', 'all_techniques.json'), 'w') as f:
                json.dump(self.collected_data['techniques'], f, indent=2)
        
        # ì„¸ê³„ ê¸°ë¡ ì €ì¥
        if self.collected_data['records']:
            df_records = pd.DataFrame(self.collected_data['records'])
            df_records.to_csv(os.path.join(self.output_dir, 'records', 'world_records.csv'), index=False)
        
        logger.info("Data saved successfully")
    
    def generate_report(self):
        """ìˆ˜ì§‘ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'collection_date': datetime.now().isoformat(),
            'total_athletes': len(self.collected_data['athletes']),
            'total_competitions': len(self.collected_data['competitions']),
            'total_techniques': len(self.collected_data['techniques']),
            'total_videos': len(self.collected_data['videos']),
            'total_records': len(self.collected_data['records']),
            'sources': list(set([item.get('source', '') for sublist in self.collected_data.values() for item in sublist if isinstance(item, dict)]))
        }
        
        # JSON ë³´ê³ ì„œ
        with open(os.path.join(self.output_dir, 'official_sports_report.json'), 'w') as f:
            json.dump(report, f, indent=2)
        
        # Markdown ë³´ê³ ì„œ
        md_report = f"""# ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ë³´ê³ ì„œ

## ğŸ“Š ìˆ˜ì§‘ í†µê³„
- **ìˆ˜ì§‘ ì¼ì‹œ**: {report['collection_date']}
- **ì´ ì„ ìˆ˜ ë°ì´í„°**: {report['total_athletes']}ëª…
- **ëŒ€íšŒ ì •ë³´**: {report['total_competitions']}ê°œ
- **ê¸°ìˆ  ìë£Œ**: {report['total_techniques']}ê°œ
- **ë¹„ë””ì˜¤ ì½˜í…ì¸ **: {report['total_videos']}ê°œ
- **ì„¸ê³„ ê¸°ë¡**: {report['total_records']}ê°œ

## ğŸŒ ë°ì´í„° ì†ŒìŠ¤
"""
        for source in report['sources']:
            md_report += f"- {source}\n"
        
        md_report += """
## âœ… ìˆ˜ì§‘ ì™„ë£Œ
ëª¨ë“  ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.
"""
        
        with open(os.path.join(self.output_dir, 'official_sports_report.md'), 'w', encoding='utf-8') as f:
            f.write(md_report)
        
        logger.info(f"Report saved to {self.output_dir}/official_sports_report.md")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = OfficialSportsScraper()
    
    # ê° ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
    await scraper.scrape_olympic_website()
    await scraper.scrape_iwf_website()
    await scraper.scrape_espn()
    await scraper.scrape_crossfit_games()
    
    # ë¯¸ë””ì–´ ì½˜í…ì¸  ë‹¤ìš´ë¡œë“œ
    # await scraper.download_media_content()  # ì„ íƒì 
    
    # ë°ì´í„° ì €ì¥
    scraper.save_collected_data()
    
    # ë³´ê³ ì„œ ìƒì„±
    scraper.generate_report()
    
    print("\nâœ… ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“ ë°ì´í„° ìœ„ì¹˜: {scraper.output_dir}")
    print(f"ğŸ“Š ìˆ˜ì§‘ í†µê³„:")
    print(f"  - ì„ ìˆ˜: {len(scraper.collected_data['athletes'])}ëª…")
    print(f"  - ëŒ€íšŒ: {len(scraper.collected_data['competitions'])}ê°œ")
    print(f"  - ê¸°ìˆ : {len(scraper.collected_data['techniques'])}ê°œ")
    print(f"  - ë¹„ë””ì˜¤: {len(scraper.collected_data['videos'])}ê°œ")


if __name__ == "__main__":
    asyncio.run(main())