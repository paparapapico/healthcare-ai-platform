"""
ìœ íŠœë¸Œì—ì„œ ì˜¬ë¦¼í”½ ì„ ìˆ˜ ìš´ë™ ì˜ìƒ ìë™ ìˆ˜ì§‘ ë° ë¶„ì„
YouTube Olympic Athletes Video Scraper and Analyzer
"""

import os
import cv2
import numpy as np
import mediapipe as mp
from pytube import YouTube, Search
import requests
from bs4 import BeautifulSoup
import json
import logging
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import asyncio
import aiohttp
from tqdm import tqdm
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OlympicDataCollector:
    """ì˜¬ë¦¼í”½ ì„ ìˆ˜ ë°ì´í„° ìë™ ìˆ˜ì§‘ê¸°"""
    
    # ìˆ˜ì§‘í•  ì˜¬ë¦¼í”½ ì„ ìˆ˜ ë° í‚¤ì›Œë“œ
    OLYMPIC_ATHLETES = {
        'weightlifting': [
            'Lasha Talakhadze',  # ì¡°ì§€ì•„, ì—­ë„ ê¸ˆë©”ë‹¬ë¦¬ìŠ¤íŠ¸
            'Shi Zhiyong',       # ì¤‘êµ­, ì—­ë„ ê¸ˆë©”ë‹¬ë¦¬ìŠ¤íŠ¸
            'Lu Xiaojun',        # ì¤‘êµ­, ì—­ë„ ì „ì„¤
            'Karlos Nasar',      # ë¶ˆê°€ë¦¬ì•„, ì‹ ì˜ˆ ì—­ë„ ì„ ìˆ˜
        ],
        'gymnastics': [
            'Simone Biles',      # ë¯¸êµ­, ì²´ì¡° GOAT
            'Kohei Uchimura',    # ì¼ë³¸, ì²´ì¡° ì™•
            'Nadia Comaneci',    # ë£¨ë§ˆë‹ˆì•„, ì²´ì¡° ì „ì„¤
        ],
        'powerlifting': [
            'Eddie Hall',        # ì˜êµ­, ìŠ¤íŠ¸ë¡±ë§¨
            'Hafthor Bjornsson', # ì•„ì´ìŠ¬ë€ë“œ, ìŠ¤íŠ¸ë¡±ë§¨
            'Brian Shaw',        # ë¯¸êµ­, ìŠ¤íŠ¸ë¡±ë§¨
        ],
        'crossfit': [
            'Mat Fraser',        # ë¯¸êµ­, í¬ë¡œìŠ¤í• ì±”í”¼ì–¸
            'Tia-Clair Toomey',  # í˜¸ì£¼, í¬ë¡œìŠ¤í• ì±”í”¼ì–¸
            'Rich Froning',      # ë¯¸êµ­, í¬ë¡œìŠ¤í• ë ˆì „ë“œ
        ]
    }
    
    # ìš´ë™ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ
    EXERCISE_KEYWORDS = {
        'squat': ['olympic squat', 'ATG squat', 'front squat', 'back squat', 'squat technique'],
        'deadlift': ['olympic deadlift', 'clean deadlift', 'snatch deadlift', 'deadlift form'],
        'bench_press': ['bench press technique', 'powerlifting bench', 'olympic bench press'],
        'clean_and_jerk': ['clean and jerk', 'olympic lifting', 'weightlifting technique'],
        'snatch': ['olympic snatch', 'snatch technique', 'weightlifting snatch']
    }
    
    def __init__(self, output_dir: str = 'olympic_dataset'):
        """
        Args:
            output_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = output_dir
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=2,
            enable_segmentation=True,
            smooth_landmarks=True,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7
        )
        
        # ë°ì´í„° ì €ì¥ ê²½ë¡œ ìƒì„±
        os.makedirs(f'{output_dir}/videos', exist_ok=True)
        os.makedirs(f'{output_dir}/annotations', exist_ok=True)
        os.makedirs(f'{output_dir}/poses', exist_ok=True)
        
        self.collected_videos = []
        self.processed_data = []
    
    async def collect_olympic_videos(self, max_videos_per_athlete: int = 5) -> List[Dict]:
        """ì˜¬ë¦¼í”½ ì„ ìˆ˜ ì˜ìƒ ìë™ ìˆ˜ì§‘"""
        logger.info("Starting Olympic athlete video collection...")
        
        all_videos = []
        
        for sport, athletes in self.OLYMPIC_ATHLETES.items():
            for athlete in athletes:
                logger.info(f"Searching for {athlete} ({sport})...")
                
                # ìš´ë™ë³„ í‚¤ì›Œë“œì™€ ì¡°í•©
                for exercise, keywords in self.EXERCISE_KEYWORDS.items():
                    for keyword in keywords[:2]:  # í‚¤ì›Œë“œë‹¹ 2ê°œì”©
                        search_query = f"{athlete} {keyword} technique analysis"
                        videos = await self.search_youtube_videos(
                            search_query, 
                            max_results=max_videos_per_athlete
                        )
                        
                        for video in videos:
                            video['athlete'] = athlete
                            video['sport'] = sport
                            video['exercise'] = exercise
                            video['quality_tier'] = 'olympic'  # ì˜¬ë¦¼í”½ ìˆ˜ì¤€
                            all_videos.append(video)
        
        # ê³µì‹ ì˜¬ë¦¼í”½ ì±„ë„ ì˜ìƒë„ ìˆ˜ì§‘
        official_channels = [
            'Olympics',
            'International Weightlifting Federation',
            'FIG Channel',
            'IPF Powerlifting'
        ]
        
        for channel in official_channels:
            for exercise, keywords in self.EXERCISE_KEYWORDS.items():
                search_query = f"{channel} {keywords[0]} slow motion"
                videos = await self.search_youtube_videos(
                    search_query,
                    max_results=10
                )
                
                for video in videos:
                    video['channel'] = channel
                    video['exercise'] = exercise
                    video['quality_tier'] = 'official'
                    all_videos.append(video)
        
        self.collected_videos = all_videos
        logger.info(f"Collected {len(all_videos)} Olympic-level videos")
        
        return all_videos
    
    async def search_youtube_videos(self, query: str, max_results: int = 10) -> List[Dict]:
        """YouTube ì˜ìƒ ê²€ìƒ‰"""
        try:
            search = Search(query)
            videos = []
            
            for i, video in enumerate(search.results[:max_results]):
                try:
                    video_info = {
                        'title': video.title,
                        'url': video.watch_url,
                        'video_id': video.video_id,
                        'duration': video.length,
                        'views': video.views,
                        'channel': video.author,
                        'description': video.description[:500] if video.description else '',
                        'search_query': query,
                        'quality_score': self.estimate_video_quality(video)
                    }
                    videos.append(video_info)
                except Exception as e:
                    logger.warning(f"Error processing video: {e}")
                    continue
            
            return videos
            
        except Exception as e:
            logger.error(f"YouTube search failed for '{query}': {e}")
            return []
    
    def estimate_video_quality(self, video) -> float:
        """ì˜ìƒ í’ˆì§ˆ ì ìˆ˜ ì¶”ì •"""
        score = 50.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ì¡°íšŒìˆ˜ ê¸°ë°˜ ì ìˆ˜
        if video.views > 1000000:
            score += 20
        elif video.views > 100000:
            score += 10
        elif video.views > 10000:
            score += 5
        
        # ì±„ë„ ì‹ ë¢°ë„
        trusted_channels = ['Olympics', 'IWF', 'IPF', 'CrossFit']
        if any(channel in video.author for channel in trusted_channels):
            score += 15
        
        # ì œëª© í‚¤ì›Œë“œ
        quality_keywords = ['technique', 'analysis', 'slow motion', 'olympic', 'world record']
        for keyword in quality_keywords:
            if keyword.lower() in video.title.lower():
                score += 3
        
        # ì˜ìƒ ê¸¸ì´ (ì ì ˆí•œ ê¸¸ì´)
        if 60 <= video.length <= 600:  # 1-10ë¶„
            score += 10
        
        return min(100, score)
    
    def download_and_process_video(self, video_info: Dict, output_path: str) -> Dict:
        """ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° í¬ì¦ˆ ì¶”ì¶œ"""
        try:
            logger.info(f"Processing: {video_info['title']}")
            
            # YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ
            yt = YouTube(video_info['url'])
            
            # ìµœê³  í’ˆì§ˆ ìŠ¤íŠ¸ë¦¼ ì„ íƒ (720p ì´ìƒ ê¶Œì¥)
            stream = yt.streams.filter(
                progressive=True, 
                file_extension='mp4'
            ).order_by('resolution').desc().first()
            
            if not stream:
                stream = yt.streams.filter(file_extension='mp4').first()
            
            if not stream:
                logger.warning(f"No suitable stream found for {video_info['title']}")
                return None
            
            # ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
            video_path = stream.download(
                output_path=f"{self.output_dir}/videos",
                filename=f"{video_info['video_id']}.mp4"
            )
            
            # í¬ì¦ˆ ì¶”ì¶œ ë° ë¶„ì„
            pose_data = self.extract_poses_from_video(video_path, video_info)
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            if self.validate_pose_data(pose_data):
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                metadata = {
                    'video_info': video_info,
                    'video_path': video_path,
                    'pose_data_path': f"{self.output_dir}/poses/{video_info['video_id']}.json",
                    'extraction_date': datetime.now().isoformat(),
                    'total_frames': len(pose_data),
                    'valid_frames': sum(1 for p in pose_data if p['landmarks']),
                    'quality_metrics': self.calculate_quality_metrics(pose_data)
                }
                
                # í¬ì¦ˆ ë°ì´í„° ì €ì¥
                with open(metadata['pose_data_path'], 'w') as f:
                    json.dump(pose_data, f, indent=2)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                with open(f"{self.output_dir}/annotations/{video_info['video_id']}.json", 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                logger.info(f"âœ… Successfully processed: {video_info['title']}")
                return metadata
            else:
                logger.warning(f"Poor quality data from: {video_info['title']}")
                # í’ˆì§ˆì´ ë‚®ì€ ë¹„ë””ì˜¤ëŠ” ì‚­ì œ
                os.remove(video_path)
                return None
                
        except Exception as e:
            logger.error(f"Failed to process video {video_info['url']}: {e}")
            return None
    
    def extract_poses_from_video(self, video_path: str, video_info: Dict) -> List[Dict]:
        """ë¹„ë””ì˜¤ì—ì„œ í¬ì¦ˆ ë°ì´í„° ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        pose_data = []
        frame_count = 0
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        pbar = tqdm(total=total_frames, desc="Extracting poses")
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # RGB ë³€í™˜
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # í¬ì¦ˆ ê°ì§€
            results = self.pose.process(rgb_frame)
            
            frame_data = {
                'frame_id': frame_count,
                'timestamp': frame_count / fps,
                'video_id': video_info['video_id'],
                'athlete': video_info.get('athlete', 'Unknown'),
                'exercise': video_info.get('exercise', 'Unknown'),
                'landmarks': None,
                'world_landmarks': None,
                'visibility_score': 0
            }
            
            if results.pose_landmarks:
                # 2D ëœë“œë§ˆí¬
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.append({
                        'x': landmark.x,
                        'y': landmark.y,
                        'z': landmark.z,
                        'visibility': landmark.visibility
                    })
                frame_data['landmarks'] = landmarks
                
                # 3D ì›”ë“œ ëœë“œë§ˆí¬
                if results.pose_world_landmarks:
                    world_landmarks = []
                    for landmark in results.pose_world_landmarks.landmark:
                        world_landmarks.append({
                            'x': landmark.x,
                            'y': landmark.y,
                            'z': landmark.z,
                            'visibility': landmark.visibility
                        })
                    frame_data['world_landmarks'] = world_landmarks
                
                # ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚°
                frame_data['visibility_score'] = np.mean([l['visibility'] for l in landmarks])
                
                # ìš´ë™ ë‹¨ê³„ ìë™ ë¼ë²¨ë§
                frame_data['phase'] = self.detect_exercise_phase(landmarks, video_info.get('exercise'))
                
                # í’ˆì§ˆ ì ìˆ˜ ì¶”ì •
                frame_data['quality_score'] = self.estimate_form_quality(landmarks, video_info.get('exercise'))
            
            pose_data.append(frame_data)
            frame_count += 1
            pbar.update(1)
        
        cap.release()
        pbar.close()
        
        return pose_data
    
    def detect_exercise_phase(self, landmarks: List[Dict], exercise: str) -> str:
        """ìš´ë™ ë‹¨ê³„ ìë™ ê°ì§€"""
        if not landmarks or len(landmarks) < 33:
            return 'unknown'
        
        # MediaPipe ëœë“œë§ˆí¬ ì¸ë±ìŠ¤
        LEFT_SHOULDER = 11
        RIGHT_SHOULDER = 12
        LEFT_ELBOW = 13
        RIGHT_ELBOW = 14
        LEFT_WRIST = 15
        RIGHT_WRIST = 16
        LEFT_HIP = 23
        RIGHT_HIP = 24
        LEFT_KNEE = 25
        RIGHT_KNEE = 26
        LEFT_ANKLE = 27
        RIGHT_ANKLE = 28
        
        if exercise in ['squat', 'front_squat', 'back_squat']:
            # ë¬´ë¦ ê°ë„ë¡œ ìŠ¤ì¿¼íŠ¸ ë‹¨ê³„ íŒë‹¨
            knee_angle = self.calculate_angle(
                landmarks[LEFT_HIP],
                landmarks[LEFT_KNEE],
                landmarks[LEFT_ANKLE]
            )
            
            if knee_angle > 160:
                return 'standing'
            elif knee_angle > 120:
                return 'descent'
            elif knee_angle > 70:
                return 'bottom'
            else:
                return 'ascent'
                
        elif exercise in ['bench_press']:
            # íŒ”ê¿ˆì¹˜ ê°ë„ë¡œ ë²¤ì¹˜í”„ë ˆìŠ¤ ë‹¨ê³„ íŒë‹¨
            elbow_angle = self.calculate_angle(
                landmarks[LEFT_SHOULDER],
                landmarks[LEFT_ELBOW],
                landmarks[LEFT_WRIST]
            )
            
            if elbow_angle > 160:
                return 'lockout'
            elif elbow_angle > 90:
                return 'descent'
            elif elbow_angle > 70:
                return 'bottom'
            else:
                return 'press'
                
        elif exercise in ['deadlift', 'clean_and_jerk', 'snatch']:
            # ì—‰ë©ì´ ë†’ì´ë¡œ ë°ë“œë¦¬í”„íŠ¸ ë‹¨ê³„ íŒë‹¨
            hip_height = (landmarks[LEFT_HIP]['y'] + landmarks[RIGHT_HIP]['y']) / 2
            knee_height = (landmarks[LEFT_KNEE]['y'] + landmarks[RIGHT_KNEE]['y']) / 2
            
            if hip_height < knee_height:
                return 'lockout'
            elif hip_height < knee_height + 0.1:
                return 'pull'
            else:
                return 'setup'
        
        return 'unknown'
    
    def calculate_angle(self, p1: Dict, p2: Dict, p3: Dict) -> float:
        """3ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
        v1 = np.array([p1['x'] - p2['x'], p1['y'] - p2['y']])
        v2 = np.array([p3['x'] - p2['x'], p3['y'] - p2['y']])
        
        cosine = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
        angle = np.arccos(np.clip(cosine, -1.0, 1.0))
        
        return np.degrees(angle)
    
    def estimate_form_quality(self, landmarks: List[Dict], exercise: str) -> float:
        """ì˜¬ë¦¼í”½ ìˆ˜ì¤€ ê¸°ì¤€ìœ¼ë¡œ ìì„¸ í’ˆì§ˆ í‰ê°€"""
        if not landmarks or len(landmarks) < 33:
            return 0.0
        
        score = 100.0
        
        # ê¸°ë³¸ ì²´í¬: ì£¼ìš” ê´€ì ˆì´ ë³´ì´ëŠ”ì§€
        key_joints = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]
        visibility_score = np.mean([landmarks[i]['visibility'] for i in key_joints])
        
        if visibility_score < 0.7:
            score -= 20
        
        # ìš´ë™ë³„ ì„¸ë¶€ í‰ê°€
        if exercise in ['squat', 'front_squat']:
            # ë¬´ë¦ ì •ë ¬ ì²´í¬
            left_knee_x = landmarks[25]['x']
            left_ankle_x = landmarks[27]['x']
            knee_alignment = abs(left_knee_x - left_ankle_x)
            
            if knee_alignment > 0.1:  # ë¬´ë¦ì´ ë°œëì„ ë„ˆë¬´ ë„˜ìŒ
                score -= 10
            
            # ì²™ì¶” ì •ë ¬ ì²´í¬
            nose_y = landmarks[0]['y']
            hip_y = (landmarks[23]['y'] + landmarks[24]['y']) / 2
            spine_angle = abs(nose_y - hip_y)
            
            if spine_angle < 0.3:  # ë„ˆë¬´ ì•ìœ¼ë¡œ ìˆ™ì„
                score -= 15
        
        return max(0, score)
    
    def validate_pose_data(self, pose_data: List[Dict]) -> bool:
        """í¬ì¦ˆ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        if not pose_data or len(pose_data) < 30:  # ìµœì†Œ 1ì´ˆ ë¶„ëŸ‰
            return False
        
        # ìœ íš¨í•œ í”„ë ˆì„ ë¹„ìœ¨
        valid_frames = sum(1 for p in pose_data if p['landmarks'] and p['visibility_score'] > 0.5)
        valid_ratio = valid_frames / len(pose_data)
        
        return valid_ratio > 0.7  # 70% ì´ìƒ ìœ íš¨í•´ì•¼ í•¨
    
    def calculate_quality_metrics(self, pose_data: List[Dict]) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {
            'avg_visibility': 0,
            'phase_distribution': {},
            'avg_quality_score': 0,
            'continuity_score': 0,
            'stability_score': 0
        }
        
        if not pose_data:
            return metrics
        
        # í‰ê·  ê°€ì‹œì„±
        visibility_scores = [p['visibility_score'] for p in pose_data if p['landmarks']]
        metrics['avg_visibility'] = np.mean(visibility_scores) if visibility_scores else 0
        
        # ìš´ë™ ë‹¨ê³„ ë¶„í¬
        phases = [p.get('phase', 'unknown') for p in pose_data]
        for phase in set(phases):
            metrics['phase_distribution'][phase] = phases.count(phase) / len(phases)
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜
        quality_scores = [p.get('quality_score', 0) for p in pose_data if p.get('quality_score')]
        metrics['avg_quality_score'] = np.mean(quality_scores) if quality_scores else 0
        
        # ì—°ì†ì„± ì ìˆ˜ (í”„ë ˆì„ ê°„ ë¶€ë“œëŸ¬ìš´ ì „í™˜)
        if len(pose_data) > 1:
            continuity_scores = []
            for i in range(1, len(pose_data)):
                if pose_data[i]['landmarks'] and pose_data[i-1]['landmarks']:
                    # í”„ë ˆì„ ê°„ ëœë“œë§ˆí¬ ë³€í™”ëŸ‰
                    diff = 0
                    for j in range(min(len(pose_data[i]['landmarks']), len(pose_data[i-1]['landmarks']))):
                        diff += abs(pose_data[i]['landmarks'][j]['x'] - pose_data[i-1]['landmarks'][j]['x'])
                        diff += abs(pose_data[i]['landmarks'][j]['y'] - pose_data[i-1]['landmarks'][j]['y'])
                    continuity_scores.append(1.0 / (1.0 + diff))  # ë³€í™”ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            
            metrics['continuity_score'] = np.mean(continuity_scores) if continuity_scores else 0
        
        return metrics
    
    async def collect_google_images(self, max_images: int = 100) -> List[Dict]:
        """êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì¡° ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("Collecting supplementary data from Google Images...")
        
        image_data = []
        
        # ìš´ë™ë³„ ì´ë¯¸ì§€ ê²€ìƒ‰
        search_queries = [
            "olympic weightlifting squat technique sequence",
            "powerlifting bench press form analysis",
            "olympic deadlift technique breakdown",
            "crossfit athletes perfect form",
            "olympic lifting slow motion",
        ]
        
        for query in search_queries:
            images = await self.search_google_images(query, max_images // len(search_queries))
            image_data.extend(images)
        
        logger.info(f"Collected {len(image_data)} reference images")
        return image_data
    
    async def search_google_images(self, query: str, max_results: int = 20) -> List[Dict]:
        """êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰ (êµìœ¡ ëª©ì )"""
        # ì‹¤ì œ êµ¬í˜„ì‹œ Google Custom Search API ì‚¬ìš© ê¶Œì¥
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ êµ¬ì¡°ë§Œ ì œê³µ
        
        images = []
        
        # Google Custom Search API ì‚¬ìš© ì˜ˆì‹œ
        # API_KEY = "your_api_key"
        # CX = "your_search_engine_id"
        # url = f"https://www.googleapis.com/customsearch/v1?q={query}&cx={CX}&key={API_KEY}&searchType=image"
        
        # ì„ì‹œ ë°ì´í„° êµ¬ì¡°
        for i in range(max_results):
            images.append({
                'url': f'placeholder_{i}',
                'title': f'{query} - Image {i}',
                'source': 'google_images',
                'query': query
            })
        
        return images
    
    def create_training_dataset(self) -> Dict:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±"""
        logger.info("Creating training dataset from collected data...")
        
        dataset = {
            'olympic_tier': [],  # ì˜¬ë¦¼í”½ ì„ ìˆ˜ ë°ì´í„°
            'professional_tier': [],  # í”„ë¡œ ì„ ìˆ˜ ë°ì´í„°
            'reference_tier': [],  # ì°¸ì¡° ë°ì´í„°
            'metadata': {
                'total_videos': len(self.collected_videos),
                'total_frames': 0,
                'athletes': list(set([v.get('athlete', 'Unknown') for v in self.collected_videos])),
                'exercises': list(set([v.get('exercise', 'Unknown') for v in self.collected_videos])),
                'creation_date': datetime.now().isoformat()
            }
        }
        
        # í¬ì¦ˆ ë°ì´í„° íŒŒì¼ ë¡œë“œ ë° ë¶„ë¥˜
        pose_files = os.listdir(f'{self.output_dir}/poses')
        
        for pose_file in pose_files:
            with open(f'{self.output_dir}/poses/{pose_file}', 'r') as f:
                pose_data = json.load(f)
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì°¾ê¸°
            video_id = pose_file.replace('.json', '')
            video_info = next((v for v in self.collected_videos if v['video_id'] == video_id), None)
            
            if video_info:
                # í’ˆì§ˆì— ë”°ë¼ í‹°ì–´ ë¶„ë¥˜
                if video_info.get('quality_tier') == 'olympic':
                    dataset['olympic_tier'].extend(pose_data[:1000])  # ìµœëŒ€ 1000 í”„ë ˆì„
                elif video_info.get('quality_tier') == 'official':
                    dataset['professional_tier'].extend(pose_data[:1000])
                else:
                    dataset['reference_tier'].extend(pose_data[:500])
                
                dataset['metadata']['total_frames'] += len(pose_data)
        
        # í•™ìŠµ ë°ì´í„°ì…‹ ì €ì¥
        with open(f'{self.output_dir}/training_dataset.json', 'w') as f:
            json.dump(dataset, f, indent=2)
        
        logger.info(f"âœ… Training dataset created with {dataset['metadata']['total_frames']} frames")
        
        # í†µê³„ ì¶œë ¥
        print("\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
        print(f"- ì˜¬ë¦¼í”½ í‹°ì–´: {len(dataset['olympic_tier'])} í”„ë ˆì„")
        print(f"- í”„ë¡œí˜ì…”ë„ í‹°ì–´: {len(dataset['professional_tier'])} í”„ë ˆì„")
        print(f"- ì°¸ì¡° í‹°ì–´: {len(dataset['reference_tier'])} í”„ë ˆì„")
        print(f"- ì´ ì„ ìˆ˜ ìˆ˜: {len(dataset['metadata']['athletes'])}")
        print(f"- ìš´ë™ ì¢…ë¥˜: {dataset['metadata']['exercises']}")
        
        return dataset
    
    async def run_full_pipeline(self, max_videos: int = 50):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("Starting full Olympic data collection pipeline...")
        
        # 1. YouTube ì˜ìƒ ìˆ˜ì§‘
        await self.collect_olympic_videos(max_videos_per_athlete=3)
        
        # 2. ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
        processed_count = 0
        for video in tqdm(self.collected_videos[:max_videos], desc="Processing videos"):
            result = self.download_and_process_video(video, self.output_dir)
            if result:
                self.processed_data.append(result)
                processed_count += 1
        
        # 3. êµ¬ê¸€ ì´ë¯¸ì§€ ë³´ì¡° ë°ì´í„° ìˆ˜ì§‘
        await self.collect_google_images(max_images=100)
        
        # 4. í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
        dataset = self.create_training_dataset()
        
        # 5. ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        self.generate_collection_report()
        
        logger.info(f"âœ… Pipeline complete! Processed {processed_count}/{len(self.collected_videos)} videos")
        
        return dataset
    
    def generate_collection_report(self):
        """ë°ì´í„° ìˆ˜ì§‘ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'collection_date': datetime.now().isoformat(),
            'total_videos_found': len(self.collected_videos),
            'total_videos_processed': len(self.processed_data),
            'success_rate': len(self.processed_data) / len(self.collected_videos) * 100 if self.collected_videos else 0,
            'top_athletes': [],
            'quality_distribution': {},
            'exercise_distribution': {},
            'total_storage_used_mb': 0
        }
        
        # ì„ ìˆ˜ë³„ í†µê³„
        athlete_counts = {}
        for video in self.collected_videos:
            athlete = video.get('athlete', 'Unknown')
            athlete_counts[athlete] = athlete_counts.get(athlete, 0) + 1
        
        report['top_athletes'] = sorted(athlete_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # í’ˆì§ˆ ë¶„í¬
        for data in self.processed_data:
            quality_tier = 'high' if data['quality_metrics']['avg_quality_score'] > 80 else 'medium' if data['quality_metrics']['avg_quality_score'] > 60 else 'low'
            report['quality_distribution'][quality_tier] = report['quality_distribution'].get(quality_tier, 0) + 1
        
        # ìš´ë™ ì¢…ë¥˜ ë¶„í¬
        for video in self.collected_videos:
            exercise = video.get('exercise', 'Unknown')
            report['exercise_distribution'][exercise] = report['exercise_distribution'].get(exercise, 0) + 1
        
        # ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰
        video_dir = f'{self.output_dir}/videos'
        if os.path.exists(video_dir):
            total_size = sum(os.path.getsize(os.path.join(video_dir, f)) for f in os.listdir(video_dir))
            report['total_storage_used_mb'] = total_size / 1024 / 1024
        
        # ë³´ê³ ì„œ ì €ì¥
        with open(f'{self.output_dir}/collection_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ
        md_report = f"""# ì˜¬ë¦¼í”½ ë°ì´í„° ìˆ˜ì§‘ ë³´ê³ ì„œ

## ğŸ“Š ìˆ˜ì§‘ í†µê³„
- **ìˆ˜ì§‘ ì¼ì‹œ**: {report['collection_date']}
- **ì´ ì˜ìƒ ë°œê²¬**: {report['total_videos_found']}ê°œ
- **ì²˜ë¦¬ ì™„ë£Œ**: {report['total_videos_processed']}ê°œ
- **ì„±ê³µë¥ **: {report['success_rate']:.1f}%
- **ì‚¬ìš© ìš©ëŸ‰**: {report['total_storage_used_mb']:.1f} MB

## ğŸ† Top 10 ì„ ìˆ˜
"""
        for athlete, count in report['top_athletes']:
            md_report += f"- {athlete}: {count}ê°œ ì˜ìƒ\n"
        
        md_report += f"""
## ğŸ“ˆ í’ˆì§ˆ ë¶„í¬
- ë†’ìŒ: {report['quality_distribution'].get('high', 0)}ê°œ
- ì¤‘ê°„: {report['quality_distribution'].get('medium', 0)}ê°œ
- ë‚®ìŒ: {report['quality_distribution'].get('low', 0)}ê°œ

## ğŸ‹ï¸ ìš´ë™ ì¢…ë¥˜
"""
        for exercise, count in report['exercise_distribution'].items():
            md_report += f"- {exercise}: {count}ê°œ\n"
        
        with open(f'{self.output_dir}/collection_report.md', 'w', encoding='utf-8') as f:
            f.write(md_report)
        
        logger.info(f"Report saved to {self.output_dir}/collection_report.md")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Collect Olympic athlete training data')
    parser.add_argument('--output_dir', type=str, default='olympic_dataset',
                       help='Output directory for collected data')
    parser.add_argument('--max_videos', type=int, default=50,
                       help='Maximum number of videos to process')
    parser.add_argument('--athletes_only', action='store_true',
                       help='Collect only specific athlete videos')
    
    args = parser.parse_args()
    
    # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = OlympicDataCollector(args.output_dir)
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    dataset = await collector.run_full_pipeline(max_videos=args.max_videos)
    
    print("\nâœ… ì˜¬ë¦¼í”½ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {args.output_dir}")
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°ì…‹: {args.output_dir}/training_dataset.json")
    print(f"ğŸ“ ìˆ˜ì§‘ ë³´ê³ ì„œ: {args.output_dir}/collection_report.md")


if __name__ == "__main__":
    asyncio.run(main())