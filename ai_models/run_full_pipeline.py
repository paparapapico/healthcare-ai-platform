"""
ì˜¬ë¦¼í”½ AI ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Complete Olympic AI Pipeline Runner
"""

import os
import sys
import json
import asyncio
import argparse
import logging
from datetime import datetime
from typing import Dict, List
import subprocess
import time
from tqdm import tqdm
import pandas as pd
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OlympicAIPipeline:
    """ì˜¬ë¦¼í”½ AI ì „ì²´ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ì"""
    
    def __init__(self, config_path: str = None):
        """
        Args:
            config_path: íŒŒì´í”„ë¼ì¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self.load_config(config_path)
        self.results = {}
        self.start_time = None
        self.end_time = None
    
    def load_config(self, config_path: str) -> Dict:
        """ì„¤ì • ë¡œë“œ"""
        default_config = {
            'data_collection': {
                'youtube': {
                    'enabled': True,
                    'max_videos': 100,
                    'output_dir': 'olympic_dataset'
                },
                'social_media': {
                    'enabled': True,
                    'instagram_posts': 50,
                    'tiktok_videos': 50,
                    'output_dir': 'social_media_dataset'
                },
                'official_sites': {
                    'enabled': True,
                    'output_dir': 'official_sports_dataset'
                },
                'crowdsourcing': {
                    'enabled': True,
                    'api_port': 8001
                }
            },
            'data_processing': {
                'min_quality_score': 60,
                'target_fps': 30,
                'image_size': (640, 480),
                'augmentation': True
            },
            'model_training': {
                'architecture': 'transformer',
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001,
                'exercises': ['squat', 'push_up', 'deadlift']
            },
            'optimization': {
                'target_platforms': ['android', 'ios'],
                'quantization': 'int8',
                'target_size_mb': 10
            },
            'deployment': {
                'backend_integration': True,
                'mobile_integration': True,
                'api_endpoints': True
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                custom_config = json.load(f)
                # ì¬ê·€ì ìœ¼ë¡œ ì„¤ì • ë³‘í•©
                self.merge_configs(default_config, custom_config)
        
        return default_config
    
    def merge_configs(self, default: Dict, custom: Dict):
        """ì„¤ì • ë³‘í•©"""
        for key, value in custom.items():
            if key in default and isinstance(default[key], dict) and isinstance(value, dict):
                self.merge_configs(default[key], value)
            else:
                default[key] = value
    
    async def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.start_time = datetime.now()
        logger.info("="*50)
        logger.info("ğŸš€ ì˜¬ë¦¼í”½ AI íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
        logger.info("="*50)
        
        try:
            # 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘
            if any(self.config['data_collection'].values()):
                await self.collect_data()
            
            # 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬
            await self.process_data()
            
            # 3ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ
            await self.train_models()
            
            # 4ë‹¨ê³„: ëª¨ë¸ ìµœì í™”
            await self.optimize_models()
            
            # 5ë‹¨ê³„: ë°°í¬
            await self.deploy_models()
            
            # 6ë‹¨ê³„: í…ŒìŠ¤íŠ¸
            await self.test_system()
            
            self.end_time = datetime.now()
            
            # ìµœì¢… ë³´ê³ ì„œ
            self.generate_final_report()
            
            logger.info("="*50)
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {self.end_time - self.start_time}")
            logger.info("="*50)
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def collect_data(self):
        """ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„"""
        logger.info("\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        collection_tasks = []
        
        # YouTube ì˜¬ë¦¼í”½ ë°ì´í„°
        if self.config['data_collection']['youtube']['enabled']:
            logger.info("  - YouTube ì˜¬ë¦¼í”½ ì˜ìƒ ìˆ˜ì§‘...")
            task = self.run_youtube_scraper()
            collection_tasks.append(task)
        
        # ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„°
        if self.config['data_collection']['social_media']['enabled']:
            logger.info("  - Instagram/TikTok ë°ì´í„° ìˆ˜ì§‘...")
            task = self.run_social_media_scraper()
            collection_tasks.append(task)
        
        # ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸
        if self.config['data_collection']['official_sites']['enabled']:
            logger.info("  - ê³µì‹ ìŠ¤í¬ì¸  ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘...")
            task = self.run_official_sites_scraper()
            collection_tasks.append(task)
        
        # í¬ë¼ìš°ë“œì†Œì‹± í”Œë«í¼ ì‹œì‘
        if self.config['data_collection']['crowdsourcing']['enabled']:
            logger.info("  - í¬ë¼ìš°ë“œì†Œì‹± í”Œë«í¼ ì‹œì‘...")
            self.start_crowdsourcing_platform()
        
        # ë³‘ë ¬ ì‹¤í–‰
        if collection_tasks:
            results = await asyncio.gather(*collection_tasks, return_exceptions=True)
            self.results['data_collection'] = results
        
        logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        # ìˆ˜ì§‘ í†µê³„
        self.print_collection_stats()
    
    async def run_youtube_scraper(self):
        """YouTube ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
        try:
            cmd = [
                'python',
                'ai_models/data_collection/youtube_olympic_scraper.py',
                '--max_videos', str(self.config['data_collection']['youtube']['max_videos']),
                '--output_dir', self.config['data_collection']['youtube']['output_dir']
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.info("    âœ“ YouTube ìˆ˜ì§‘ ì„±ê³µ")
                return {'status': 'success', 'source': 'youtube'}
            else:
                logger.error(f"    âœ— YouTube ìˆ˜ì§‘ ì‹¤íŒ¨: {stderr.decode()}")
                return {'status': 'failed', 'source': 'youtube', 'error': stderr.decode()}
                
        except Exception as e:
            logger.error(f"YouTube ìŠ¤í¬ë˜í¼ ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'source': 'youtube', 'error': str(e)}
    
    async def run_social_media_scraper(self):
        """ì†Œì…œ ë¯¸ë””ì–´ ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
        try:
            # social_media_scraper.pyì˜ main í•¨ìˆ˜ ì§ì ‘ ì„í¬íŠ¸
            sys.path.append('ai_models/data_collection')
            from social_media_scraper import SocialMediaScraper
            
            scraper = SocialMediaScraper(
                output_dir=self.config['data_collection']['social_media']['output_dir']
            )
            
            # Instagram Reels
            instagram_posts = await scraper.scrape_instagram_reels(
                max_posts=self.config['data_collection']['social_media']['instagram_posts']
            )
            
            # TikTok
            tiktok_videos = await scraper.scrape_tiktok_videos(
                max_videos=self.config['data_collection']['social_media']['tiktok_videos']
            )
            
            # í¬ì¦ˆ ì¶”ì¶œ
            await scraper.process_all_videos()
            
            # ë³´ê³ ì„œ ìƒì„±
            scraper.generate_report()
            
            logger.info("    âœ“ ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘ ì„±ê³µ")
            return {
                'status': 'success',
                'source': 'social_media',
                'instagram': len(instagram_posts),
                'tiktok': len(tiktok_videos)
            }
            
        except Exception as e:
            logger.error(f"ì†Œì…œ ë¯¸ë””ì–´ ìŠ¤í¬ë˜í¼ ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'source': 'social_media', 'error': str(e)}
    
    async def run_official_sites_scraper(self):
        """ê³µì‹ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
        try:
            sys.path.append('ai_models/data_collection')
            from sports_official_scraper import OfficialSportsScraper
            
            scraper = OfficialSportsScraper(
                output_dir=self.config['data_collection']['official_sites']['output_dir']
            )
            
            # ê° ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
            await scraper.scrape_olympic_website()
            await scraper.scrape_iwf_website()
            await scraper.scrape_espn()
            await scraper.scrape_crossfit_games()
            
            # ë°ì´í„° ì €ì¥
            scraper.save_collected_data()
            scraper.generate_report()
            
            logger.info("    âœ“ ê³µì‹ ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ì„±ê³µ")
            return {
                'status': 'success',
                'source': 'official_sites',
                'athletes': len(scraper.collected_data['athletes']),
                'competitions': len(scraper.collected_data['competitions'])
            }
            
        except Exception as e:
            logger.error(f"ê³µì‹ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í¼ ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'source': 'official_sites', 'error': str(e)}
    
    def start_crowdsourcing_platform(self):
        """í¬ë¼ìš°ë“œì†Œì‹± í”Œë«í¼ ì‹œì‘"""
        try:
            # ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰
            cmd = [
                'python',
                'ai_models/data_collection/crowdsource_platform.py'
            ]
            
            subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            logger.info(f"    âœ“ í¬ë¼ìš°ë“œì†Œì‹± í”Œë«í¼ ì‹œì‘ (í¬íŠ¸: {self.config['data_collection']['crowdsourcing']['api_port']})")
            
        except Exception as e:
            logger.error(f"í¬ë¼ìš°ë“œì†Œì‹± í”Œë«í¼ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    async def process_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„"""
        logger.info("\nğŸ”§ 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ëª¨ë“  ìˆ˜ì§‘ëœ ë°ì´í„° í†µí•©
        all_datasets = []
        
        # YouTube ë°ì´í„°
        youtube_dir = self.config['data_collection']['youtube']['output_dir']
        if os.path.exists(f"{youtube_dir}/training_dataset.json"):
            with open(f"{youtube_dir}/training_dataset.json", 'r') as f:
                youtube_data = json.load(f)
                all_datasets.append(('youtube', youtube_data))
        
        # ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„°
        social_dir = self.config['data_collection']['social_media']['output_dir']
        if os.path.exists(social_dir):
            # í¬ì¦ˆ ë°ì´í„° íŒŒì¼ë“¤ ìˆ˜ì§‘
            pose_files = [f for f in os.listdir(f"{social_dir}/metadata") if f.endswith('_poses.json')]
            social_data = []
            for pose_file in pose_files:
                with open(f"{social_dir}/metadata/{pose_file}", 'r') as f:
                    social_data.extend(json.load(f))
            all_datasets.append(('social_media', social_data))
        
        # ë°ì´í„° ì •ì œ ë° í†µí•©
        processed_data = await self.clean_and_merge_data(all_datasets)
        
        # ë°ì´í„° ì¦ê°•
        if self.config['data_processing']['augmentation']:
            processed_data = await self.augment_data(processed_data)
        
        # í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
        train_data, val_data, test_data = self.split_data(processed_data)
        
        # ì €ì¥
        os.makedirs('ai_models/processed_data', exist_ok=True)
        
        with open('ai_models/processed_data/train_data.json', 'w') as f:
            json.dump(train_data, f)
        
        with open('ai_models/processed_data/val_data.json', 'w') as f:
            json.dump(val_data, f)
        
        with open('ai_models/processed_data/test_data.json', 'w') as f:
            json.dump(test_data, f)
        
        self.results['data_processing'] = {
            'total_samples': len(processed_data),
            'train_samples': len(train_data),
            'val_samples': len(val_data),
            'test_samples': len(test_data)
        }
        
        logger.info(f"  - ì´ ìƒ˜í”Œ: {len(processed_data)}")
        logger.info(f"  - í•™ìŠµ: {len(train_data)}, ê²€ì¦: {len(val_data)}, í…ŒìŠ¤íŠ¸: {len(test_data)}")
        logger.info("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    
    async def clean_and_merge_data(self, datasets: List) -> List:
        """ë°ì´í„° ì •ì œ ë° ë³‘í•©"""
        merged_data = []
        
        for source, data in datasets:
            logger.info(f"  - {source} ë°ì´í„° ì •ì œ ì¤‘...")
            
            if isinstance(data, dict):
                # ì˜¬ë¦¼í”½ í‹°ì–´ ë°ì´í„° ìš°ì„ 
                if 'olympic_tier' in data:
                    merged_data.extend(data['olympic_tier'])
                if 'professional_tier' in data:
                    merged_data.extend(data['professional_tier'])
            elif isinstance(data, list):
                # í’ˆì§ˆ í•„í„°ë§
                for item in data:
                    if self.validate_data_quality(item):
                        merged_data.append(item)
        
        return merged_data
    
    def validate_data_quality(self, data_item: Dict) -> bool:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        # ìµœì†Œ í’ˆì§ˆ ì ìˆ˜
        if 'quality_score' in data_item:
            return data_item['quality_score'] >= self.config['data_processing']['min_quality_score']
        
        # ëœë“œë§ˆí¬ ì¡´ì¬ ì—¬ë¶€
        if 'landmarks' in data_item:
            return data_item['landmarks'] is not None and len(data_item['landmarks']) > 0
        
        return True
    
    async def augment_data(self, data: List) -> List:
        """ë°ì´í„° ì¦ê°•"""
        logger.info("  - ë°ì´í„° ì¦ê°• ì¤‘...")
        augmented = []
        
        for item in tqdm(data, desc="Augmenting"):
            augmented.append(item)  # ì›ë³¸
            
            # ì‹œê°„ ì›Œí•‘
            if 'landmarks' in item:
                warped = item.copy()
                warped['augmentation'] = 'time_warp'
                augmented.append(warped)
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noisy = item.copy()
            noisy['augmentation'] = 'noise'
            augmented.append(noisy)
        
        return augmented
    
    def split_data(self, data: List) -> tuple:
        """ë°ì´í„° ë¶„í• """
        np.random.shuffle(data)
        
        total = len(data)
        train_size = int(total * 0.7)
        val_size = int(total * 0.15)
        
        train_data = data[:train_size]
        val_data = data[train_size:train_size + val_size]
        test_data = data[train_size + val_size:]
        
        return train_data, val_data, test_data
    
    async def train_models(self):
        """ëª¨ë¸ í•™ìŠµ ë‹¨ê³„"""
        logger.info("\nğŸ§  3ë‹¨ê³„: AI ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        training_results = []
        
        for exercise in self.config['model_training']['exercises']:
            logger.info(f"  - {exercise} ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            cmd = [
                'python',
                'ai_models/training/train_model.py',
                '--exercise', exercise,
                '--data', 'ai_models/processed_data/train_data.json',
                '--epochs', str(self.config['model_training']['epochs']),
                '--batch_size', str(self.config['model_training']['batch_size']),
                '--architecture', self.config['model_training']['architecture'],
                '--export', 'tflite'
            ]
            
            try:
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                stdout, stderr = await process.communicate()
                
                if process.returncode == 0:
                    logger.info(f"    âœ“ {exercise} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                    training_results.append({
                        'exercise': exercise,
                        'status': 'success'
                    })
                else:
                    logger.error(f"    âœ— {exercise} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
                    training_results.append({
                        'exercise': exercise,
                        'status': 'failed',
                        'error': stderr.decode()
                    })
                    
            except Exception as e:
                logger.error(f"ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")
                training_results.append({
                    'exercise': exercise,
                    'status': 'error',
                    'error': str(e)
                })
        
        self.results['model_training'] = training_results
        logger.info("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    async def optimize_models(self):
        """ëª¨ë¸ ìµœì í™” ë‹¨ê³„"""
        logger.info("\nâš¡ 4ë‹¨ê³„: ëª¨ë¸ ìµœì í™” ì‹œì‘...")
        
        optimization_results = []
        
        for exercise in self.config['model_training']['exercises']:
            model_path = f'models/best_{exercise}_model.h5'
            
            if not os.path.exists(model_path):
                logger.warning(f"  - {exercise} ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
                continue
            
            logger.info(f"  - {exercise} ëª¨ë¸ ìµœì í™” ì¤‘...")
            
            cmd = [
                'python',
                'ai_models/optimization/mobile_optimizer.py',
                '--model', model_path,
                '--platform', 'all',
                '--output', f'optimization_report_{exercise}.json'
            ]
            
            try:
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                stdout, stderr = await process.communicate()
                
                if process.returncode == 0:
                    logger.info(f"    âœ“ {exercise} ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
                    optimization_results.append({
                        'exercise': exercise,
                        'status': 'success'
                    })
                else:
                    logger.error(f"    âœ— {exercise} ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨")
                    optimization_results.append({
                        'exercise': exercise,
                        'status': 'failed'
                    })
                    
            except Exception as e:
                logger.error(f"ëª¨ë¸ ìµœì í™” ì˜¤ë¥˜: {e}")
                optimization_results.append({
                    'exercise': exercise,
                    'status': 'error',
                    'error': str(e)
                })
        
        self.results['optimization'] = optimization_results
        logger.info("âœ… ëª¨ë¸ ìµœì í™” ì™„ë£Œ!")
    
    async def deploy_models(self):
        """ëª¨ë¸ ë°°í¬ ë‹¨ê³„"""
        logger.info("\nğŸš€ 5ë‹¨ê³„: ëª¨ë¸ ë°°í¬ ì‹œì‘...")
        
        deployment_tasks = []
        
        # ë°±ì—”ë“œ API í†µí•©
        if self.config['deployment']['backend_integration']:
            logger.info("  - ë°±ì—”ë“œ API í†µí•©...")
            task = self.integrate_backend_api()
            deployment_tasks.append(task)
        
        # ëª¨ë°”ì¼ ì•± í†µí•©
        if self.config['deployment']['mobile_integration']:
            logger.info("  - ëª¨ë°”ì¼ ì•± í†µí•©...")
            task = self.integrate_mobile_app()
            deployment_tasks.append(task)
        
        if deployment_tasks:
            results = await asyncio.gather(*deployment_tasks, return_exceptions=True)
            self.results['deployment'] = results
        
        logger.info("âœ… ëª¨ë¸ ë°°í¬ ì™„ë£Œ!")
    
    async def integrate_backend_api(self):
        """ë°±ì—”ë“œ API í†µí•©"""
        try:
            # ìµœì í™”ëœ ëª¨ë¸ì„ ë°±ì—”ë“œë¡œ ë³µì‚¬
            os.makedirs('backend/app/ai_models/deployed', exist_ok=True)
            
            for exercise in self.config['model_training']['exercises']:
                tflite_path = f'optimized_models/tflite/model_{exercise}.tflite'
                if os.path.exists(tflite_path):
                    import shutil
                    shutil.copy(
                        tflite_path,
                        f'backend/app/ai_models/deployed/{exercise}_model.tflite'
                    )
            
            logger.info("    âœ“ ë°±ì—”ë“œ API í†µí•© ì™„ë£Œ")
            return {'status': 'success', 'component': 'backend'}
            
        except Exception as e:
            logger.error(f"ë°±ì—”ë“œ í†µí•© ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'component': 'backend', 'error': str(e)}
    
    async def integrate_mobile_app(self):
        """ëª¨ë°”ì¼ ì•± í†µí•©"""
        try:
            # ìµœì í™”ëœ ëª¨ë¸ì„ ëª¨ë°”ì¼ ì•±ìœ¼ë¡œ ë³µì‚¬
            os.makedirs('mobile/assets/models', exist_ok=True)
            
            for exercise in self.config['model_training']['exercises']:
                tflite_path = f'optimized_models/tflite/model_{exercise}.tflite'
                if os.path.exists(tflite_path):
                    import shutil
                    shutil.copy(
                        tflite_path,
                        f'mobile/assets/models/{exercise}_model.tflite'
                    )
            
            logger.info("    âœ“ ëª¨ë°”ì¼ ì•± í†µí•© ì™„ë£Œ")
            return {'status': 'success', 'component': 'mobile'}
            
        except Exception as e:
            logger.error(f"ëª¨ë°”ì¼ í†µí•© ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'component': 'mobile', 'error': str(e)}
    
    async def test_system(self):
        """ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        logger.info("\nğŸ§ª 6ë‹¨ê³„: ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
        
        test_results = {
            'backend_api': False,
            'mobile_app': False,
            'ai_inference': False,
            'crowdsourcing': False
        }
        
        # ë°±ì—”ë“œ API í…ŒìŠ¤íŠ¸
        try:
            import requests
            response = requests.get('http://localhost:8000/api/v1/health')
            test_results['backend_api'] = response.status_code == 200
            logger.info(f"  - ë°±ì—”ë“œ API: {'âœ“' if test_results['backend_api'] else 'âœ—'}")
        except:
            logger.info("  - ë°±ì—”ë“œ API: âœ—")
        
        # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        try:
            tflite_path = 'optimized_models/tflite/model_squat.tflite'
            if os.path.exists(tflite_path):
                import tensorflow as tf
                interpreter = tf.lite.Interpreter(model_path=tflite_path)
                interpreter.allocate_tensors()
                test_results['ai_inference'] = True
            logger.info(f"  - AI ì¶”ë¡ : {'âœ“' if test_results['ai_inference'] else 'âœ—'}")
        except:
            logger.info("  - AI ì¶”ë¡ : âœ—")
        
        # í¬ë¼ìš°ë“œì†Œì‹± í”Œë«í¼ í…ŒìŠ¤íŠ¸
        try:
            import requests
            response = requests.get('http://localhost:8001/stats')
            test_results['crowdsourcing'] = response.status_code == 200
            logger.info(f"  - í¬ë¼ìš°ë“œì†Œì‹±: {'âœ“' if test_results['crowdsourcing'] else 'âœ—'}")
        except:
            logger.info("  - í¬ë¼ìš°ë“œì†Œì‹±: âœ—")
        
        self.results['testing'] = test_results
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = sum(test_results.values()) / len(test_results) * 100
        logger.info(f"\ní…ŒìŠ¤íŠ¸ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if success_rate >= 75:
            logger.info("âœ… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            logger.warning("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - í™•ì¸ í•„ìš”")
    
    def print_collection_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        if 'data_collection' not in self.results:
            return
        
        logger.info("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ í†µê³„:")
        
        total_videos = 0
        total_frames = 0
        
        for result in self.results['data_collection']:
            if isinstance(result, dict) and result.get('status') == 'success':
                source = result.get('source', 'unknown')
                
                if source == 'youtube':
                    logger.info(f"  - YouTube: ìˆ˜ì§‘ ì™„ë£Œ")
                elif source == 'social_media':
                    logger.info(f"  - Instagram: {result.get('instagram', 0)}ê°œ")
                    logger.info(f"  - TikTok: {result.get('tiktok', 0)}ê°œ")
                elif source == 'official_sites':
                    logger.info(f"  - ì„ ìˆ˜ ë°ì´í„°: {result.get('athletes', 0)}ëª…")
                    logger.info(f"  - ëŒ€íšŒ ë°ì´í„°: {result.get('competitions', 0)}ê°œ")
    
    def generate_final_report(self):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'pipeline_start': self.start_time.isoformat(),
            'pipeline_end': self.end_time.isoformat(),
            'duration': str(self.end_time - self.start_time),
            'results': self.results,
            'config': self.config
        }
        
        # JSON ë³´ê³ ì„œ
        with open('pipeline_report.json', 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # Markdown ë³´ê³ ì„œ
        md_report = f"""# ì˜¬ë¦¼í”½ AI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë³´ê³ ì„œ

## ğŸ“… ì‹¤í–‰ ì •ë³´
- **ì‹œì‘ ì‹œê°„**: {self.start_time}
- **ì¢…ë£Œ ì‹œê°„**: {self.end_time}
- **ì´ ì†Œìš” ì‹œê°„**: {self.end_time - self.start_time}

## ğŸ“Š ì‹¤í–‰ ê²°ê³¼

### 1. ë°ì´í„° ìˆ˜ì§‘
"""
        
        if 'data_collection' in self.results:
            for result in self.results['data_collection']:
                if isinstance(result, dict):
                    md_report += f"- **{result.get('source', 'unknown')}**: {result.get('status', 'unknown')}\n"
        
        if 'data_processing' in self.results:
            md_report += f"""
### 2. ë°ì´í„° ì „ì²˜ë¦¬
- **ì´ ìƒ˜í”Œ**: {self.results['data_processing'].get('total_samples', 0)}
- **í•™ìŠµ ë°ì´í„°**: {self.results['data_processing'].get('train_samples', 0)}
- **ê²€ì¦ ë°ì´í„°**: {self.results['data_processing'].get('val_samples', 0)}
- **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: {self.results['data_processing'].get('test_samples', 0)}
"""
        
        if 'model_training' in self.results:
            md_report += "\n### 3. ëª¨ë¸ í•™ìŠµ\n"
            for result in self.results['model_training']:
                md_report += f"- **{result['exercise']}**: {result['status']}\n"
        
        if 'testing' in self.results:
            test_results = self.results['testing']
            md_report += f"""
### 4. ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
- **ë°±ì—”ë“œ API**: {'âœ…' if test_results.get('backend_api') else 'âŒ'}
- **AI ì¶”ë¡ **: {'âœ…' if test_results.get('ai_inference') else 'âŒ'}
- **í¬ë¼ìš°ë“œì†Œì‹±**: {'âœ…' if test_results.get('crowdsourcing') else 'âŒ'}
"""
        
        md_report += """
## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„
1. ëª¨ë°”ì¼ ì•±ì—ì„œ í…ŒìŠ¤íŠ¸
2. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
3. ëª¨ë¸ ì§€ì†ì  ê°œì„ 
"""
        
        with open('pipeline_report.md', 'w', encoding='utf-8') as f:
            f.write(md_report)
        
        logger.info(f"\nğŸ“„ ë³´ê³ ì„œ ì €ì¥: pipeline_report.md")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Olympic AI Pipeline Runner')
    parser.add_argument('--config', type=str, help='Configuration file path')
    parser.add_argument('--skip-collection', action='store_true', help='Skip data collection')
    parser.add_argument('--skip-training', action='store_true', help='Skip model training')
    parser.add_argument('--skip-deployment', action='store_true', help='Skip deployment')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = OlympicAIPipeline(config_path=args.config)
    
    # ì˜µì…˜ì— ë”°ë¼ ë‹¨ê³„ ë¹„í™œì„±í™”
    if args.skip_collection:
        for key in pipeline.config['data_collection']:
            pipeline.config['data_collection'][key]['enabled'] = False
    
    if args.skip_training:
        pipeline.config['model_training']['epochs'] = 0
    
    if args.skip_deployment:
        pipeline.config['deployment']['backend_integration'] = False
        pipeline.config['deployment']['mobile_integration'] = False
    
    await pipeline.run_full_pipeline()


if __name__ == "__main__":
    print("""
    ======================================================
         Olympic AI Full Pipeline Runner
    ======================================================
    
      1. Data Collection (YouTube, SNS, Official Sites)
      2. Data Processing & Augmentation
      3. AI Model Training (Transformer)
      4. Mobile Optimization (TFLite, CoreML)
      5. App Integration & Deployment
      6. System Testing
    
    ======================================================
    """)
    
    asyncio.run(main())