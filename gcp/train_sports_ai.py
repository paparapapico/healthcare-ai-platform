"""
GCPì—ì„œ ì‹¤í–‰í•  Sports AI í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
Main Training Script for Sports AI on GCP
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, Dataset
import wandb
from google.cloud import storage
import time
from datetime import datetime
import argparse
import json
from pathlib import Path
import numpy as np
from tqdm import tqdm
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GCPTrainingManager:
    """GCP í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.scaler = GradScaler()  # Mixed Precision
        
        # GCS ì„¤ì •
        self.bucket_name = 'sports-ai-data-bucket'
        self.checkpoint_bucket = 'sports-ai-data-bucket-checkpoints'
        self.model_bucket = 'sports-ai-data-bucket-models'
        
        # WandB ì´ˆê¸°í™”
        if not args.no_wandb:
            wandb.init(
                project="sports-ai-training",
                name=f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                config=vars(args)
            )
        
        logger.info(f"ğŸš€ í•™ìŠµ ì‹œì‘ - Device: {self.device}")
        self._log_system_info()
    
    def _log_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…"""
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            logger.info(f"CUDA: {torch.version.cuda}")
            logger.info(f"cuDNN: {torch.backends.cudnn.version()}")
    
    def train_phase_1_experiments(self):
        """Phase 1: T4ë¡œ ì‹¤í—˜ ë° í…ŒìŠ¤íŠ¸ ($50 ì˜ˆì‚°)"""
        logger.info("="*50)
        logger.info("ğŸ“Š Phase 1: ì‹¤í—˜ ë‹¨ê³„ ì‹œì‘ (Tesla T4)")
        logger.info("="*50)
        
        # ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œì‘
        experiments = [
            {
                'name': 'lightweight_model',
                'batch_size': 16,
                'learning_rate': 1e-4,
                'epochs': 10,
                'data_fraction': 0.01  # 1% ë°ì´í„°
            },
            {
                'name': 'medium_model',
                'batch_size': 8,
                'learning_rate': 5e-5,
                'epochs': 10,
                'data_fraction': 0.05  # 5% ë°ì´í„°
            },
            {
                'name': 'transfer_learning',
                'batch_size': 12,
                'learning_rate': 2e-5,
                'epochs': 5,
                'data_fraction': 0.1  # 10% ë°ì´í„°
            }
        ]
        
        best_config = None
        best_accuracy = 0
        
        for exp in experiments:
            logger.info(f"\nğŸ”¬ ì‹¤í—˜: {exp['name']}")
            
            # ëª¨ë¸ ìƒì„±
            model = self._create_model(exp['name'])
            
            # ë°ì´í„° ë¡œë”
            train_loader, val_loader = self._prepare_data(
                batch_size=exp['batch_size'],
                data_fraction=exp['data_fraction']
            )
            
            # í•™ìŠµ
            accuracy = self._train_model(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                epochs=exp['epochs'],
                learning_rate=exp['learning_rate'],
                experiment_name=exp['name']
            )
            
            # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_config = exp
                self._save_checkpoint(model, f"best_phase1_{exp['name']}.pth")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            torch.cuda.empty_cache()
            
            # ë¹„ìš© ì¶”ì 
            self._log_cost_estimate('t4', hours=2)
        
        logger.info(f"\nâœ… Phase 1 ì™„ë£Œ!")
        logger.info(f"ìµœê³  ì„¤ì •: {best_config['name']}")
        logger.info(f"ìµœê³  ì •í™•ë„: {best_accuracy:.2%}")
        
        return best_config
    
    def train_phase_2_full_training(self, best_config):
        """Phase 2: V100ìœ¼ë¡œ ë³¸ê²© í•™ìŠµ ($200 ì˜ˆì‚°)"""
        logger.info("="*50)
        logger.info("ğŸš€ Phase 2: ë³¸ê²© í•™ìŠµ (Tesla V100)")
        logger.info("="*50)
        
        # ìŠ¤í¬ì¸ ë³„ í•™ìŠµ
        sports = ['basketball', 'soccer', 'bodyweight', 'golf']
        trained_models = {}
        
        for sport in sports:
            logger.info(f"\nğŸƒ {sport.upper()} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            
            # ì „ì²´ ë°ì´í„° ì‚¬ìš©
            model = self._create_model('full_model')
            
            # ìŠ¤í¬ì¸ ë³„ ë°ì´í„° ë¡œë“œ
            train_loader, val_loader = self._prepare_sport_data(
                sport=sport,
                batch_size=32,  # V100ì€ ë” í° ë°°ì¹˜ ê°€ëŠ¥
                data_fraction=1.0  # 100% ë°ì´í„°
            )
            
            # í•™ìŠµ
            accuracy = self._train_model(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                epochs=50,
                learning_rate=1e-4,
                experiment_name=f"{sport}_full"
            )
            
            # ëª¨ë¸ ì €ì¥
            model_path = f"models/{sport}_model_v100.pth"
            self._save_to_gcs(model, model_path)
            trained_models[sport] = accuracy
            
            logger.info(f"âœ… {sport} ì™„ë£Œ: {accuracy:.2%}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            torch.cuda.empty_cache()
            
            # ë¹„ìš© ì¶”ì 
            self._log_cost_estimate('v100', hours=18)
        
        logger.info(f"\nâœ… Phase 2 ì™„ë£Œ!")
        for sport, acc in trained_models.items():
            logger.info(f"{sport}: {acc:.2%}")
        
        return trained_models
    
    def train_phase_3_optimization(self, trained_models):
        """Phase 3: ìµœì¢… ìµœì í™” ë° ë°°í¬ ì¤€ë¹„ ($50 ì˜ˆì‚°)"""
        logger.info("="*50)
        logger.info("âš¡ Phase 3: ìµœì í™” ë‹¨ê³„ (Tesla T4)")
        logger.info("="*50)
        
        optimization_tasks = [
            'model_ensemble',
            'quantization',
            'tensorrt_optimization',
            'mobile_conversion'
        ]
        
        for task in optimization_tasks:
            logger.info(f"\nğŸ”§ {task} ì§„í–‰ ì¤‘...")
            
            if task == 'model_ensemble':
                ensemble_model = self._create_ensemble(trained_models)
                self._save_to_gcs(ensemble_model, "models/ensemble_model.pth")
                
            elif task == 'quantization':
                for sport in trained_models.keys():
                    quantized = self._quantize_model(f"models/{sport}_model_v100.pth")
                    self._save_to_gcs(quantized, f"models/{sport}_quantized.pth")
            
            elif task == 'tensorrt_optimization':
                self._optimize_tensorrt(trained_models)
            
            elif task == 'mobile_conversion':
                self._convert_to_mobile(trained_models)
            
            # ë¹„ìš© ì¶”ì 
            self._log_cost_estimate('t4', hours=3)
        
        logger.info("\nâœ… Phase 3 ì™„ë£Œ!")
        logger.info("ëª¨ë“  ëª¨ë¸ì´ ë°°í¬ ì¤€ë¹„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    def _create_model(self, model_type):
        """ëª¨ë¸ ìƒì„±"""
        if model_type == 'lightweight_model':
            from ai_models.models.unified_sports_ai_model import UnifiedSportsAIModel
            model = UnifiedSportsAIModel(
                sports_types=['basketball', 'soccer'],
                architecture='lightweight'
            ).build_unified_model()
        
        elif model_type == 'full_model':
            from ai_models.models.unified_sports_ai_model import UnifiedSportsAIModel
            model = UnifiedSportsAIModel(
                sports_types=['basketball', 'soccer', 'bodyweight', 'golf'],
                architecture='hybrid_ensemble'
            ).build_unified_model()
        
        else:  # transfer_learning
            # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ
            model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ ìˆ˜ì •
            model.fc = nn.Linear(2048, 100)  # 100ê°œ í´ë˜ìŠ¤
        
        return model.to(self.device)
    
    def _train_model(self, model, train_loader, val_loader, epochs, learning_rate, experiment_name):
        """ëª¨ë¸ í•™ìŠµ"""
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        best_accuracy = 0
        
        for epoch in range(epochs):
            # Training
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
            for batch_idx, (inputs, targets) in enumerate(pbar):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                # Mixed Precision Training
                with autocast():
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                
                # Backward
                self.scaler.scale(loss).backward()
                
                # Gradient Accumulation
                if (batch_idx + 1) % self.args.accumulation_steps == 0:
                    self.scaler.step(optimizer)
                    self.scaler.update()
                    optimizer.zero_grad()
                
                # Statistics
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += targets.size(0)
                train_correct += predicted.eq(targets).sum().item()
                
                pbar.set_postfix({
                    'loss': train_loss/(batch_idx+1),
                    'acc': 100.*train_correct/train_total
                })
            
            # Validation
            accuracy = self._validate(model, val_loader)
            
            # Logging
            if not self.args.no_wandb:
                wandb.log({
                    f'{experiment_name}/train_loss': train_loss/len(train_loader),
                    f'{experiment_name}/train_acc': 100.*train_correct/train_total,
                    f'{experiment_name}/val_acc': accuracy,
                    'epoch': epoch
                })
            
            # Save best model
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                self._save_checkpoint(model, f"{experiment_name}_best.pth")
            
            # Preemptible ëŒ€ë¹„ ì£¼ê¸°ì  ì €ì¥
            if epoch % 5 == 0:
                self._save_checkpoint(model, f"{experiment_name}_epoch_{epoch}.pth")
        
        return best_accuracy
    
    def _validate(self, model, val_loader):
        """ê²€ì¦"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        accuracy = 100. * correct / total
        logger.info(f"Validation Accuracy: {accuracy:.2f}%")
        return accuracy
    
    def _save_checkpoint(self, model, filename):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'timestamp': datetime.now().isoformat(),
            'cuda_device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu'
        }
        
        # ë¡œì»¬ ì €ì¥
        local_path = Path('checkpoints') / filename
        local_path.parent.mkdir(exist_ok=True)
        torch.save(checkpoint, local_path)
        
        # GCS ì—…ë¡œë“œ
        self._upload_to_gcs(local_path, f"checkpoints/{filename}")
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filename}")
    
    def _save_to_gcs(self, model, path):
        """GCSì— ëª¨ë¸ ì €ì¥"""
        local_path = Path(path)
        local_path.parent.mkdir(exist_ok=True)
        torch.save(model.state_dict(), local_path)
        self._upload_to_gcs(local_path, path)
    
    def _upload_to_gcs(self, local_path, gcs_path):
        """GCS ì—…ë¡œë“œ"""
        client = storage.Client()
        bucket = client.bucket(self.checkpoint_bucket)
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(str(local_path))
        logger.info(f"â˜ï¸ GCS ì—…ë¡œë“œ: gs://{self.checkpoint_bucket}/{gcs_path}")
    
    def _log_cost_estimate(self, gpu_type, hours):
        """ë¹„ìš© ì¶”ì • ë¡œê¹…"""
        costs = {
            't4': 0.10,  # Preemptible T4
            'v100': 2.48  # Regular V100
        }
        cost = costs.get(gpu_type, 0) * hours
        total_cost = getattr(self, 'total_cost', 0) + cost
        self.total_cost = total_cost
        
        logger.info(f"ğŸ’° ë¹„ìš©: ${cost:.2f} (ëˆ„ì : ${total_cost:.2f}/$300)")
        
        if not self.args.no_wandb:
            wandb.log({
                'cost/session': cost,
                'cost/total': total_cost,
                'cost/remaining': 300 - total_cost
            })
    
    def _prepare_data(self, batch_size, data_fraction):
        """ë°ì´í„° ì¤€ë¹„ (ê°„ë‹¨í•œ ì˜ˆì‹œ)"""
        # ì‹¤ì œë¡œëŠ” GCSì—ì„œ ë°ì´í„° ë¡œë“œ
        from torch.utils.data import TensorDataset
        
        # ë”ë¯¸ ë°ì´í„° (ì‹¤ì œë¡œëŠ” collected_sports_data ì‚¬ìš©)
        n_samples = int(10000 * data_fraction)
        X = torch.randn(n_samples, 3, 224, 224)
        y = torch.randint(0, 10, (n_samples,))
        
        dataset = TensorDataset(X, y)
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        return train_loader, val_loader
    
    def _prepare_sport_data(self, sport, batch_size, data_fraction):
        """ìŠ¤í¬ì¸ ë³„ ë°ì´í„° ì¤€ë¹„"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìˆ˜ì§‘ëœ ë°ì´í„° ì‚¬ìš©
        return self._prepare_data(batch_size, data_fraction)
    
    def run_complete_training(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # Phase 1: ì‹¤í—˜
            best_config = self.train_phase_1_experiments()
            
            # Phase 2: ë³¸ê²© í•™ìŠµ
            trained_models = self.train_phase_2_full_training(best_config)
            
            # Phase 3: ìµœì í™”
            self.train_phase_3_optimization(trained_models)
            
            # ì™„ë£Œ
            total_time = (time.time() - start_time) / 3600
            logger.info("="*50)
            logger.info("ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ!")
            logger.info(f"â±ï¸ ì´ ì‹œê°„: {total_time:.1f}ì‹œê°„")
            logger.info(f"ğŸ’° ì´ ë¹„ìš©: ${self.total_cost:.2f}")
            logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: gs://{self.model_bucket}/")
            logger.info("="*50)
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self._save_checkpoint(None, "error_checkpoint.pth")
            raise


def main():
    parser = argparse.ArgumentParser(description='Sports AI Training on GCP')
    parser.add_argument('--phase', type=str, default='all', choices=['1', '2', '3', 'all'])
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    parser.add_argument('--checkpoint', type=str, help='Checkpoint path')
    parser.add_argument('--accumulation_steps', type=int, default=8)
    parser.add_argument('--no_wandb', action='store_true', help='Disable WandB logging')
    
    args = parser.parse_args()
    
    # í•™ìŠµ ì‹œì‘
    trainer = GCPTrainingManager(args)
    
    if args.phase == 'all':
        trainer.run_complete_training()
    elif args.phase == '1':
        trainer.train_phase_1_experiments()
    elif args.phase == '2':
        best_config = {'name': 'transfer_learning'}  # ê¸°ë³¸ê°’
        trainer.train_phase_2_full_training(best_config)
    elif args.phase == '3':
        trained_models = {}  # ê¸°ë³¸ê°’
        trainer.train_phase_3_optimization(trained_models)


if __name__ == "__main__":
    main()